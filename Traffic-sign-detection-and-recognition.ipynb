{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from tensorflow.keras.models import load_model\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "from skimage import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to process detected label image\n",
    "def process_images(image):\n",
    "    \"\"\" the function resizes & normalises detected image\n",
    "    then return it as a numpy array\n",
    "    \"\"\"\n",
    "    # resize the image to be 32x32 pixels, ignoring aspect ratio\n",
    "    img = transform.resize(image, (32,32))\n",
    "    img = exposure.equalize_adapthist(img, clip_limit=0.1)\n",
    "    \n",
    "    # preprocess the image by scaling it to the range [0, 1]\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    \n",
    "    # convert the data and labels to NumPy arrays\n",
    "    image = np.expand_dims(img, axis=0)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "recognition_model = load_model('trained_model/sign_recognition_final_model/model.pb')\n",
    "\n",
    "yolo_model = cv2.dnn.readNetFromDarknet('trained_model/cov_yolov4.cfg','trained_model/cov_yolov4_2000.weights')\n",
    "# Get all layers from the yolo network\n",
    "yolo_layers = yolo_model.getLayerNames()\n",
    "# Loop and find the last layer (output layer) of the yolo network \n",
    "yolo_output_layer = [yolo_layers[yolo_layer[0] - 1] for yolo_layer in yolo_model.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the label names\n",
    "labelNames = open(\"signnames.csv\").read().strip().split(\"\\n\")[1:]\n",
    "labelNames = [l.split(\",\")[1] for l in labelNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load video\n",
    "video = cv2.VideoCapture('test.mp4')\n",
    "\n",
    "#create a while loop \n",
    "while (video.isOpened):\n",
    "    #get the current frame from video stream\n",
    "    ret,frame = video.read()\n",
    "    if ret == True:\n",
    "        height = frame.shape[0]\n",
    "        width = frame.shape[1]\n",
    "\n",
    "        # convert to blob to pass into model\n",
    "        #recommended by yolo authors, scale factor is 0.003922=1/255, width,height of blob is 320,320\n",
    "        #accepted sizes are 320×320,416×416,609×609. More size means more accuracy but less speed\n",
    "        img_blob = cv2.dnn.blobFromImage(frame, 0.003922, (416, 416), swapRB=True, crop=False)\n",
    "        \n",
    "        # pass the blob to Yolo model\n",
    "        yolo_model.setInput(img_blob)\n",
    "\n",
    "        # get the detection from YOLO model using forward()\n",
    "        obj_detection_layers = yolo_model.forward(yolo_output_layer)\n",
    "       \n",
    "        # initialization for non-max suppression (NMS)\n",
    "        # declare list for [class id], [box center, width & height[], [confidences]\n",
    "        class_ids_list = []\n",
    "        boxes_list = []\n",
    "        confidences_list = []\n",
    "\n",
    "        # loop over each of the layer outputs\n",
    "        for object_detection_layer in obj_detection_layers:\n",
    "            # loop over the detections\n",
    "            for object_detection in object_detection_layer:\n",
    "                i = 0\n",
    "                # obj_detections[1 to 4] => will have the two center points, box width and box height\n",
    "                # obj_detections[5] => will have scores for all objects within bounding box\n",
    "                all_scores = object_detection[5:]\n",
    "                predicted_class_id = np.argmax(all_scores)\n",
    "                prediction_confidence = all_scores[predicted_class_id]\n",
    "\n",
    "                # take only predictions with confidence more than 70%\n",
    "                if prediction_confidence > 0.70:\n",
    "                    #get the predicted label\n",
    "                    predicted_class_label = class_labels[predicted_class_id]\n",
    "                    #obtain the bounding box co-oridnates for actual image from resized image size\n",
    "                    bounding_box = object_detection[0:4] * np.array([width, height, width, height])\n",
    "                    (box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
    "                    start_x_pt = int(box_center_x_pt - (box_width / 2))\n",
    "                    start_y_pt = int(box_center_y_pt - (box_height / 2))\n",
    "\n",
    "                    class_ids_list.append(predicted_class_id)\n",
    "                    confidences_list.append(float(prediction_confidence))\n",
    "                    boxes_list.append([start_x_pt, start_y_pt, int(box_width), int(box_height)])\n",
    "\n",
    "        # Applying the NMS will return only the selected max value ids while suppressing the non maximum (weak) overlapping bounding boxes      \n",
    "        # Non-Maxima Suppression confidence set as 0.5 & max_suppression threhold for NMS as 0.4 (adjust and try for better perfomance)\n",
    "        max_value_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, 0.5, 0.4)\n",
    "\n",
    "        # loop through the final set of detections remaining after NMS and draw bounding box and write text\n",
    "        for max_valueid in max_value_ids:\n",
    "            max_class_id = max_valueid[0]\n",
    "            box = boxes_list[max_class_id]\n",
    "            start_x_pt = box[0]\n",
    "            start_y_pt = box[1]\n",
    "            box_width = box[2]\n",
    "            box_height = box[3]\n",
    "\n",
    "            #get the predicted class id and label\n",
    "            predicted_class_id = class_ids_list[max_class_id]\n",
    "            predicted_class_label = class_labels[predicted_class_id]\n",
    "            prediction_confidence = confidences_list[max_class_id]\n",
    "\n",
    "            end_x_pt = start_x_pt + box_width\n",
    "            end_y_pt = start_y_pt + box_height\n",
    "            \n",
    "            # crop image to the boundary box\n",
    "            croped_img = frame[start_y_pt : end_y_pt, start_x_pt : end_x_pt]\n",
    "            if croped_img.shape[0] != 0 and croped_img.shape[1] != 0:\n",
    "                croped_img = process_images(croped_img)\n",
    "                preds = recognition_model.predict(croped_img)\n",
    "                j = preds.argmax(axis=1)[0]\n",
    "                label = labelNames[j]\n",
    "\n",
    "            # draw rectangle and text in the image\n",
    "            cv2.rectangle(frame, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), [0,255,0], 1)\n",
    "            cv2.putText(frame, label, (start_x_pt, start_y_pt-5), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, [0, 255,0], 1)\n",
    "            \n",
    "        cv2.imshow(\"Detection Output\", frame)\n",
    "    \n",
    "        #terminate while loop if 'q' key is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        #cv2.destroyAllWindows()\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
